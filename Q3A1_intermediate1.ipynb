{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    s = 1/(1+np.exp(-a))\n",
    "    return s\n",
    "\n",
    "def derivative_sigmoid(a):\n",
    "    ds = sigmoid(a) *(1-sigmoid (a))\n",
    "    return ds\n",
    "\n",
    "def tanh(a):\n",
    "    t=(np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "    return t\n",
    "\n",
    "def derivative_tanh(a):\n",
    "    dt=1-tanh(a)**2\n",
    "    return dt\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    return np.exp(a) / np.sum(np.exp(a), axis=0) #expA (axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dimensions):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        parameters['w' + str(k)] = np.random.randn(layer_dimensions[k], layer_dimensions[k-1]) \n",
    "        parameters['b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "#         assert(parameters['w' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "#         assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_agrregation(h, w, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    h -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    w -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- the input of the activation function, also called pre-activation parameter \n",
    "    temp -- a python tuple containing \"h\", \"w\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.dot(w, h) + b\n",
    "\n",
    "    #assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    temp = (h,w,b)\n",
    "    \n",
    "    return a ,temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(h_prev, w, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    h_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    w -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    h -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "\n",
    "        a, linear_temp = linear_agrregation(h_prev, w, b)\n",
    "        h = sigmoid(a)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        a, linear_temp = linear_agrregation(h_prev, w, b)\n",
    "        h = tanh(a)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        a, linear_temp = linear_agrregation(h_prev, w, b)\n",
    "        h = softmax(a)\n",
    "    \n",
    "    \n",
    "#     assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    temp = (linear_temp, a)\n",
    "\n",
    "    return h, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    temps = []\n",
    "    h = x\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for k in range(L-1):\n",
    "        l = k+1\n",
    "        h_prev = h \n",
    "        h,temp = activation(h_prev, parameters['w'+str(l)], parameters['b'+str(l)], activation=\"sigmoid\")\n",
    "        temps.append(temp)\n",
    "    \n",
    "    \n",
    "    hL,temp1 = activation(h, parameters['w'+str(L)], parameters['b'+str(L)], activation=\"softmax\")\n",
    "    temps.append(temp1)\n",
    "    \n",
    "    #assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return hL, temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(yhat, y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    hL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = y.shape[1] # no. of examples\n",
    "  \n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    product_sum = np.sum((y *np.log(yhat)), axis = 0)\n",
    "    cost = -1/m*np.sum(product_sum)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    #assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dL_da, temp):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    h_prev, w, b = temp\n",
    "    m = h_prev.shape[1]\n",
    "\n",
    "    dL_dh_prev = np.dot(w.T, dL_da)\n",
    "    \n",
    "    dL_dw = 1/m*np.dot(dL_da, h_prev.T)\n",
    "    dL_db = 1/m*np.sum(dL_da, axis=1, keepdims=True)\n",
    "     \n",
    "    \n",
    "#     assert (dA_prev.shape == A_prev.shape)\n",
    "#     assert (dW.shape == W.shape)\n",
    "#     assert (db.shape == b.shape)\n",
    "    \n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dL_dh, temp, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dL_dh -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dL_dh_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_temp, a = temp\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        ds = derivative_sigmoid(a)\n",
    "        dL_da = dL_dh * ds\n",
    "       \n",
    "        dL_dh_prev, dL_dw, dL_db = linear_backward(dL_da, linear_temp)    \n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dt = derivative_tanh(a)\n",
    "        dL_da = dL_dh * dt\n",
    "\n",
    "        dL_dh_prev, dL_dw, dL_db = linear_backward(dL_da, linear_temp)    \n",
    "    \n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(yhat, y, temps):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l],\n",
    "                for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(temps) # the number of layers\n",
    "    m = y.shape[1]\n",
    "#     Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "\n",
    "# el = one hot vector\n",
    "    el = y\n",
    "    dL_dyhat = -(1/yhat)*el\n",
    "    dL_daL  = -(el - yhat)\n",
    "    current_temp = temps[L-1]\n",
    "    linear_tempL,aL = current_temp\n",
    "    \n",
    "    hL_prev, wL, bL = linear_tempL\n",
    "    m = hL_prev.shape[1]\n",
    "\n",
    "    dL_dhL_prev = np.dot(wL.T, dL_daL)\n",
    "    \n",
    "    dL_dwL = 1/m*np.dot(dL_daL, hL_prev.T)\n",
    "    dL_dbL = 1/m*np.sum(dL_daL, axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "    grads[\"dL_dh\" + str(L-1)] = dL_dhL_prev\n",
    "    grads[\"dL_dw\" + str(L)]      = dL_dwL\n",
    "    grads[\"dL_db\" + str(L)] = dL_dbL\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        #print(l)\n",
    "        current_temp = temps[l]\n",
    "        dL_dh_prev, dL_dw, dL_db = linear_activation_backward(grads[\"dL_dh\" + str(l+1)], current_temp, \"sigmoid\")\n",
    "        grads[\"dL_dh\" + str(l)] = dL_dh_prev\n",
    "        grads[\"dL_dw\" + str(l + 1)] = dL_dw\n",
    "        grads[\"dL_db\" + str(l + 1)] = dL_db\n",
    "        #print(grads)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]-learning_rate*grads[\"dL_dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"dL_db\" + str(l + 1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_orig = x_train_orig[:1000]\n",
    "x_test_orig = x_test_orig[:50]\n",
    "y_train_orig = y_train_orig[:1000]\n",
    "y_test_orig = y_train_orig[:50]\n",
    "y_train_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape: (784, 1000)\n",
      "x_test's shape: (784, 50)\n"
     ]
    }
   ],
   "source": [
    "x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1).T\n",
    "x_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1).T\n",
    "x_train = x_train_flatten/255\n",
    "x_test = x_test_flatten/255\n",
    "\n",
    "print (\"x_train's shape: \" + str(x_train.shape))\n",
    "print (\"x_test's shape: \" + str(x_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 9. It's a Ankle boot picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 15\n",
    "plt.imshow(x_train_orig[index])\n",
    "print (\"y = \" + str(y_train_orig[index])+ \". It's a \" + class_names[y_train_orig[index]] +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hidden_layers = 3 # no of hidden layers\n",
    "no_neuron_hidden = 784 # no. of neurons in each hidden layers\n",
    "no_neuron_output = 10 # # no. of neurons in each hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_training_examples = np.shape(x_train)[1]\n",
    "no_of_testing_examples = np.shape(x_test)[1]\n",
    "size_input_layer = np.shape(x_train)[0]\n",
    "size_hidden_layer = no_neuron_hidden\n",
    "size_output_layer = no_neuron_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_orig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector_form(labels,size_output_layer):\n",
    "    no_of_examples = labels.shape[0]\n",
    "    one_hot_vector = np.zeros((no_of_examples , size_output_layer))\n",
    "    for i in range(no_of_examples):\n",
    "        one_hot_vector[i, labels[i]] = 1    \n",
    "        y = one_hot_vector.T\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = one_hot_vector_form(y_train_orig,size_output_layer)\n",
    "y_test = one_hot_vector_form(y_test_orig,size_output_layer)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 784, 784, 784, 10]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dimensions = [size_input_layer]+ [size_hidden_layer]*no_hidden_layers+ [size_output_layer]\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(x, y, layer_dimensions, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "   \n",
    "    parameters = initialize_parameters(layer_dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "       \n",
    "        yhat, temps = forward_pass(x, parameters)\n",
    " \n",
    "        cost = cost_function(yhat, y)\n",
    "        \n",
    "        grads = backward_pass(yhat,y,temps)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        #print(\"cost in iteration \",i,\" is =\",cost)\n",
    "                \n",
    "        \n",
    "            # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 25.970778\n",
      "Cost after iteration 100: 3.301585\n",
      "Cost after iteration 200: 1.541633\n",
      "Cost after iteration 300: 0.792404\n",
      "Cost after iteration 400: 0.437510\n",
      "Cost after iteration 500: 0.261699\n",
      "Cost after iteration 600: 0.165699\n",
      "Cost after iteration 700: 0.111106\n",
      "Cost after iteration 800: 0.078888\n",
      "Cost after iteration 900: 0.058887\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(x_train, y_train, layer_dimensions, num_iterations = 1000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    prob, temps = forward_pass(x, parameters)\n",
    "    \n",
    "\n",
    "    predicted_label = np.argmax(prob, axis=0)\n",
    "    true_label = np.argmax(y, axis=0)\n",
    "    \n",
    "    Accuracy = np.sum(predicted_label == true_label)/1000\n",
    "        \n",
    "#         if probas[0,i] > 0.5:\n",
    "#             p[0,i] = 1\n",
    "#         else:\n",
    "#             p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(Accuracy))\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.998\n"
     ]
    }
   ],
   "source": [
    "predict(x_train,y_train,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.006\n"
     ]
    }
   ],
   "source": [
    "predict(x_test,y_test,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
