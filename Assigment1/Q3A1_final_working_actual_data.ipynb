{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "35I0Rq_ALBn1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "95tMwn2GLBn-"
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    s = 1/(1+np.exp(-a))\n",
    "    return s\n",
    "\n",
    "def derivative_sigmoid(a):\n",
    "    ds = sigmoid(a) *(1-sigmoid (a))\n",
    "    return ds\n",
    "\n",
    "def tanh(a):\n",
    "    t=(np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "    return t\n",
    "\n",
    "def derivative_tanh(a):\n",
    "    dt=1-tanh(a)**2\n",
    "    return dt\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    return np.exp(a) / np.sum(np.exp(a), axis=0) #expA (axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jPo8y7emLBn_"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaktHRmbLBn_",
    "outputId": "d34bec94-2a04-4235-81ff-4f96c7c87ead"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector_form(labels,no_neurons_output_layer):\n",
    "    no_of_examples = labels.shape[0]\n",
    "    one_hot_vector = np.zeros((no_of_examples , no_neurons_output_layer))\n",
    "    for i in range(no_of_examples):\n",
    "        one_hot_vector[i, labels[i]] = 1    \n",
    "        y = one_hot_vector#.T\n",
    "    return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Pgc8UpKtLBn_"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        parameters['w' + str(k)] = np.random.randn(layer_dimensions[k], layer_dimensions[k-1]) \n",
    "        parameters['b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_update(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    update = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        update['update_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        update['update_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    velocity = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        velocity['v_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        velocity['v_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_moment(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    moment = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        moment['m_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        moment['m_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "44uvQtw7LBoA"
   },
   "outputs": [],
   "source": [
    "def agrregation_forward(h, w, b):\n",
    "    \n",
    "    a = np.dot(w, h) + b\n",
    "    temp = (h,w,b)\n",
    "    \n",
    "    return a ,temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ynL63aRULBoA"
   },
   "outputs": [],
   "source": [
    "def activation_forward(h_prev, w, b, activation):\n",
    "        \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = sigmoid(a)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = tanh(a)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = softmax(a)\n",
    "    \n",
    "    \n",
    "    temp = (linear_temp, a)\n",
    "\n",
    "    return h, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d7MK5JmxLBoB"
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, parameters):\n",
    "\n",
    "    temps = []\n",
    "    h = x\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for k in range(L-1):\n",
    "        l = k+1\n",
    "        h_prev = h \n",
    "        h,temp = activation_forward(h_prev, parameters['w'+str(l)], parameters['b'+str(l)], activation=\"sigmoid\")\n",
    "        temps.append(temp)\n",
    "    \n",
    "    \n",
    "    hL,temp1 = activation_forward(h, parameters['w'+str(L)], parameters['b'+str(L)], activation=\"softmax\")\n",
    "    temps.append(temp1)\n",
    "    \n",
    "            \n",
    "    return hL, temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OX6qxo5yLBoB"
   },
   "outputs": [],
   "source": [
    "def cost_function(yhat, y):   \n",
    "    m = y.shape[1] # no. of examples\n",
    "  \n",
    "    product_sum = np.sum((y *np.log(yhat)), axis = 0)\n",
    "    cost = -1/m*np.sum(product_sum)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4sEmnKQzLBoB"
   },
   "outputs": [],
   "source": [
    "def agrregation_backward(dL_da, temp):\n",
    "    \n",
    "    h_prev, w, b = temp \n",
    "    m = h_prev.shape[1]\n",
    "    dL_dh_prev = np.dot(w.T, dL_da)\n",
    "    \n",
    "    dL_dw = 1/m*np.dot(dL_da, h_prev.T)\n",
    "    dL_db = 1/m*np.sum(dL_da, axis=1, keepdims=True)\n",
    "     \n",
    "\n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k5_8wzplLBoC"
   },
   "outputs": [],
   "source": [
    "def activation_backward(dL_dh, temp, activation):\n",
    "\n",
    "    linear_temp, a = temp\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        ds = derivative_sigmoid(a)\n",
    "        dL_da = dL_dh * ds\n",
    "       \n",
    "        dL_dh_prev, dL_dw, dL_db = agrregation_backward(dL_da, linear_temp)    \n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dt = derivative_tanh(a)\n",
    "        dL_da = dL_dh * dt\n",
    "\n",
    "        dL_dh_prev, dL_dw, dL_db = agrregation_backward(dL_da, linear_temp)    \n",
    "    \n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Izs4-_QvLBoC"
   },
   "outputs": [],
   "source": [
    "def backward_pass(yhat, y, temps):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(temps) # the number of layers\n",
    "    m = y.shape[1]\n",
    "\n",
    "# el = one hot vector\n",
    "    el = y\n",
    "    dL_dyhat = -(1/yhat)*el\n",
    "    dL_daL  = -(el - yhat)\n",
    "    current_temp = temps[L-1]\n",
    "    linear_tempL,aL = current_temp\n",
    "    \n",
    "    hL_prev, wL, bL = linear_tempL\n",
    "    m = hL_prev.shape[1]\n",
    "\n",
    "    dL_dhL_prev = np.dot(wL.T, dL_daL)\n",
    "    \n",
    "    dL_dwL = 1/m*np.dot(dL_daL, hL_prev.T)\n",
    "    dL_dbL = 1/m*np.sum(dL_daL, axis=1, keepdims=True)\n",
    "\n",
    "    grads[\"dL_dh\" + str(L-1)] = dL_dhL_prev\n",
    "    grads[\"dL_dw\" + str(L)]      = dL_dwL\n",
    "    grads[\"dL_db\" + str(L)] = dL_dbL\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        #print(l)\n",
    "        current_temp = temps[l]\n",
    "        dL_dh_prev, dL_dw, dL_db = activation_backward(grads[\"dL_dh\" + str(l+1)], current_temp, \"sigmoid\")\n",
    "        grads[\"dL_dh\" + str(l)] = dL_dh_prev\n",
    "        grads[\"dL_dw\" + str(l + 1)] = dL_dw\n",
    "        grads[\"dL_db\" + str(l + 1)] = dL_db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SzwkIK5CLBoD"
   },
   "outputs": [],
   "source": [
    "def parameter_update_vanilla(parameters, grads,learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- learning_rate*grads[\"dL_dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- learning_rate*grads[\"dL_db\" + str(l + 1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_momentum(parameters, grads, update, learning_rate ,gamma):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        update[\"update_w\" + str(l+1)] = gamma*update[\"update_w\" + str(l+1)] + learning_rate*grads[\"dL_dw\" + str(l + 1)]\n",
    "        update[\"update_b\" + str(l+1)] = gamma*update[\"update_b\" + str(l+1)] + learning_rate*grads[\"dL_db\" + str(l + 1)]\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]-update[\"update_w\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- update[\"update_b\" + str(l+1)]\n",
    "\n",
    "    return parameters, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_RMSProp(parameters, grads, velocity, learning_rate ,beta,eps):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        velocity[\"v_w\" + str(l+1)] = beta*velocity[\"v_w\" + str(l+1)] + (1-beta)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta*velocity[\"v_b\" + str(l+1)] + (1-beta)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*grads[\"dL_dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*grads[\"dL_db\" + str(l + 1)]\n",
    "\n",
    "    return parameters, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_adam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,epoch):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        \n",
    "        moment[\"m_w\" + str(l+1)] = beta1*moment[\"m_w\" + str(l+1)] + (1-beta1)*grads[\"dL_dw\" + str(l + 1)]\n",
    "        moment[\"m_b\" + str(l+1)] = beta1*moment[\"m_b\" + str(l+1)] + (1-beta1)*grads[\"dL_db\" + str(l + 1)]\n",
    "           \n",
    "        velocity[\"v_w\" + str(l+1)] = beta2*velocity[\"v_w\" + str(l+1)] + (1-beta2)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta2*velocity[\"v_b\" + str(l+1)] + (1-beta2)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        m_w_hat = moment[\"m_w\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        m_b_hat = moment[\"m_b\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        \n",
    "        v_w_hat = velocity[\"v_w\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        v_b_hat = velocity[\"v_b\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- ((learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*m_w_hat)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- ((learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*m_b_hat)\n",
    "\n",
    "    return parameters, velocity, moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_nadam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,epoch):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        \n",
    "        moment[\"m_w\" + str(l+1)] = beta1*moment[\"m_w\" + str(l+1)] + (1-beta1)*grads[\"dL_dw\" + str(l + 1)]\n",
    "        moment[\"m_b\" + str(l+1)] = beta1*moment[\"m_b\" + str(l+1)] + (1-beta1)*grads[\"dL_db\" + str(l + 1)]\n",
    "           \n",
    "        velocity[\"v_w\" + str(l+1)] = beta2*velocity[\"v_w\" + str(l+1)] + (1-beta2)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta2*velocity[\"v_b\" + str(l+1)] + (1-beta2)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        m_w_hat = moment[\"m_w\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        m_b_hat = moment[\"m_b\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        \n",
    "        v_w_hat = velocity[\"v_w\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        v_b_hat = velocity[\"v_b\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        \n",
    "        \n",
    "        nadam_update_w =  (beta1*m_w_hat) + (((1-beta1)*grads[\"dL_dw\" + str(l + 1)])/ (1-beta1**(epoch+1)))\n",
    "        nadam_update_b =  (beta1*m_b_hat) + (((1-beta1)*grads[\"dL_db\" + str(l + 1)])/ (1-beta1**(epoch+1)))\n",
    "\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- ((learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*nadam_update_w)                                       \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- ((learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*nadam_update_b)\n",
    "\n",
    "    return parameters, velocity, moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lookahead_parameters(parameters,update,gamma):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    lookahead_parameters = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        lookahead_parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]-gamma*update[\"update_w\" + str(l+1)] \n",
    "        lookahead_parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-gamma*update[\"update_b\" + str(l+1)]\n",
    "    return lookahead_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iDKqDY1dLBoD"
   },
   "outputs": [],
   "source": [
    "def predict(x, y, parameters):\n",
    "    x =x.T\n",
    "    y =y.T\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    prob, temps = forward_pass(x, parameters)\n",
    "    \n",
    "\n",
    "    predicted_label = np.argmax(prob, axis=0)\n",
    "    true_label = np.argmax(y, axis=0)\n",
    "    \n",
    "    Accuracy = np.sum(predicted_label == true_label)/m\n",
    "        \n",
    "#         if probas[0,i] > 0.5:\n",
    "#             p[0,i] = 1\n",
    "#         else:\n",
    "#             p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(Accuracy))\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RK_ze5mDLBoD",
    "outputId": "c7e09113-b4de-4448-d25c-92df6bdcdd7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_orig = x_train_orig[:5000]\n",
    "x_test_orig = x_test_orig[:1000]\n",
    "y_train_orig = y_train_orig[:5000]\n",
    "y_test_orig = y_train_orig[:1000]\n",
    "y_train_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vqejUVSLBoE",
    "outputId": "7198b220-8fdd-4216-c443-326e179f4330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape: (5000, 784)\n",
      "x_test's shape: (1000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1)\n",
    "x_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1)\n",
    "x_train = x_train_flatten/255\n",
    "x_test = x_test_flatten/255\n",
    "\n",
    "print (\"x_train's shape: \" + str(x_train.shape))\n",
    "print (\"x_test's shape: \" + str(x_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MSTq1e4SLBoE"
   },
   "outputs": [],
   "source": [
    "# index = 15\n",
    "# plt.imshow(x_train_orig[index])\n",
    "# print (\"y = \" + str(y_train_orig[index])+ \". It's a \" + class_names[y_train_orig[index]] +  \" picture.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = np.array([[1,2,3,4],[3,4,5,6],[5,6,7,8]])\n",
    "y_train = np.array([[0,0,1],[1,0,0],[0,1,0]])\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WVOCfRCQLBoG"
   },
   "outputs": [],
   "source": [
    "no_of_training_examples = np.shape(x_train)[0]\n",
    "no_of_testing_examples = np.shape(x_test)[0]\n",
    "size_input_layer = [x_train.shape[1]]\n",
    "no_hidden_layers = 3\n",
    "size_hidden_layers = [32,32,32]\n",
    "no_neurons_output_layer = len(class_names) # no of class labels\n",
    "size_output_layer = [no_neurons_output_layer]   #no_neuron_output = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkU692HSLBoG",
    "outputId": "8db703a3-d2ba-46ec-ce95-07d531c7909d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = one_hot_vector_form(y_train_orig,no_neurons_output_layer)\n",
    "y_test = one_hot_vector_form(y_test_orig,no_neurons_output_layer)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNHTywuvLBoH",
    "outputId": "2c44494e-bce9-45f2-ee3d-658f8acae3af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 32, 32, 32, 10]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dimensions = size_input_layer + size_hidden_layers + size_output_layer\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tFIBmzk1LBoH"
   },
   "outputs": [],
   "source": [
    "def L_layer_network(x_train, y_train,layer_dimensions,learning_rate,num_epochs,gamma = 0.9,  \n",
    "                    batch_type =\"Full_batch\",batchsize = 2,grad_descent_type = \"Vanilla\", beta_rms = 0.9, \n",
    "                    beta1 = 0.9,beta2 = 0.999,eps = 1e-8, print_cost=False):\n",
    "    \n",
    "    print(learning_rate)\n",
    "\n",
    "    #np.random.seed(1)\n",
    "    costs = []          \n",
    "    \n",
    "    parameters = initialize_parameters(layer_dimensions)\n",
    "    update = initialize_update(layer_dimensions)\n",
    "    velocity = initialize_velocity(layer_dimensions)\n",
    "    moment = initialize_moment(layer_dimensions)\n",
    "    if batch_type == \"SGD\":\n",
    "        batchsize =1\n",
    "    elif batch_type ==  \"Mini_batch\":\n",
    "        batchsize = batchsize\n",
    "    elif batch_type == \"Full_batch\":\n",
    "        batchsize = x_train.shape[0]\n",
    "        \n",
    "    total_examples = x_train.shape[0]\n",
    "    num_steps = total_examples//batchsize\n",
    "    #print(num_steps)\n",
    " \n",
    "    for i in range(0, num_epochs):\n",
    "        #print(\"***********epoch = \",i)\n",
    "        par_update = 0\n",
    "        for j in range(num_steps):\n",
    "            \n",
    "            start = j*batchsize\n",
    "            end = start+batchsize\n",
    "            x = x_train[start:end].T\n",
    "            y = y_train[start:end].T \n",
    "            \n",
    "            #print(\"param\",parameters)\n",
    "            #print(\"in update\",update)\n",
    "               \n",
    "            if grad_descent_type == \"Vanilla\":\n",
    "            \n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters = parameter_update_vanilla(parameters, grads,learning_rate)\n",
    "                \n",
    "            elif grad_descent_type == \"Momentum\":\n",
    "                \n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                #print(\"in update\",update)\n",
    "                parameters,update = parameter_update_momentum(parameters, grads, update, learning_rate ,gamma)\n",
    "                #print(\"out update\",update)\n",
    "                \n",
    "            elif grad_descent_type == \"NAG\":\n",
    "                lookahead_parameters = find_lookahead_parameters(parameters,update,gamma)\n",
    "                #print(\"lookahead_parameters\",lookahead_parameters)\n",
    "                yhat, temps = forward_pass(x, lookahead_parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,update = parameter_update_momentum(parameters, grads, update, learning_rate ,gamma)\n",
    "                \n",
    "            elif grad_descent_type == \"RMSProp\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity= parameter_update_RMSProp(parameters, grads, velocity,learning_rate ,beta_rms,eps)\n",
    "                #print(\"velocity\",velocity)\n",
    "                \n",
    "            elif grad_descent_type == \"Adam\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity, moment = parameter_update_adam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,i)\n",
    "                #print(\"velocity\",velocity)\n",
    "\n",
    "            elif grad_descent_type == \"Nadam\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity, moment = parameter_update_nadam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,i)\n",
    "                #print(\"velocity\",velocity)   \n",
    "\n",
    "            #print(\"out update\",update)\n",
    "            par_update += 1\n",
    "        \n",
    "        #print(\"par_updated \",par_update,\"times\")\n",
    "        #print(\"***********************************************************\")\n",
    "        \n",
    "    #     #print(\"cost in iteration \",i,\" is =\",cost)\n",
    "                \n",
    "            # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4gK1XfLLBoI",
    "outputId": "aaf77a51-a907-4e68-e1b3-5d3793c1400f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "Cost after iteration 0: 5.373005\n",
      "Cost after iteration 100: 0.931055\n",
      "Cost after iteration 200: 0.593819\n",
      "Cost after iteration 300: 0.480575\n",
      "Cost after iteration 400: 0.409962\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_network(x_train, y_train, layer_dimensions,0.001,500,grad_descent_type = \"Nadam\",print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6ZaDOFXELBoI",
    "outputId": "f4dd0bb2-b713-474c-d4b2-e449417426cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "predict(x_train, y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Tg0V3J9RLBoJ",
    "outputId": "3ec70517-246f-4afb-df38-6a503a21c787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.094\n"
     ]
    }
   ],
   "source": [
    "predict(x_test,y_test,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GopICBHzLBoJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q3A1_final_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
