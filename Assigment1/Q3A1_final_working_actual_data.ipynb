{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "35I0Rq_ALBn1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrishi_m\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "95tMwn2GLBn-"
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    s = 1/(1+np.exp(-a))\n",
    "    return s\n",
    "\n",
    "def derivative_sigmoid(a):\n",
    "    ds = sigmoid(a) *(1-sigmoid (a))\n",
    "    return ds\n",
    "\n",
    "def tanh(a):\n",
    "    t=(np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "    return t\n",
    "\n",
    "def derivative_tanh(a):\n",
    "    dt=1-tanh(a)**2\n",
    "    return dt\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    return np.exp(a) / np.sum(np.exp(a), axis=0) #expA (axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jPo8y7emLBn_"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaktHRmbLBn_",
    "outputId": "d34bec94-2a04-4235-81ff-4f96c7c87ead"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector_form(labels,no_neurons_output_layer):\n",
    "    no_of_examples = labels.shape[0]\n",
    "    one_hot_vector = np.zeros((no_of_examples , no_neurons_output_layer))\n",
    "    for i in range(no_of_examples):\n",
    "        one_hot_vector[i, labels[i]] = 1    \n",
    "        y = one_hot_vector#.T\n",
    "    return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Pgc8UpKtLBn_"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        parameters['w' + str(k)] = np.random.randn(layer_dimensions[k], layer_dimensions[k-1]) \n",
    "        parameters['b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_update(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    update = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        update['update_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        update['update_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    velocity = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        velocity['v_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        velocity['v_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_moment(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    moment = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        moment['m_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        moment['m_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "44uvQtw7LBoA"
   },
   "outputs": [],
   "source": [
    "def agrregation_forward(h, w, b):\n",
    "    \n",
    "    a = np.dot(w, h) + b\n",
    "    temp = (h,w,b)\n",
    "    \n",
    "    return a ,temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ynL63aRULBoA"
   },
   "outputs": [],
   "source": [
    "def activation_forward(h_prev, w, b, activation):\n",
    "        \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = sigmoid(a)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = tanh(a)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = softmax(a)\n",
    "    \n",
    "    \n",
    "    temp = (linear_temp, a)\n",
    "\n",
    "    return h, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d7MK5JmxLBoB"
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, parameters):\n",
    "\n",
    "    temps = []\n",
    "    h = x\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for k in range(L-1):\n",
    "        l = k+1\n",
    "        h_prev = h \n",
    "        h,temp = activation_forward(h_prev, parameters['w'+str(l)], parameters['b'+str(l)], activation=\"sigmoid\")\n",
    "        temps.append(temp)\n",
    "    \n",
    "    \n",
    "    hL,temp1 = activation_forward(h, parameters['w'+str(L)], parameters['b'+str(L)], activation=\"softmax\")\n",
    "    temps.append(temp1)\n",
    "    \n",
    "            \n",
    "    return hL, temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OX6qxo5yLBoB"
   },
   "outputs": [],
   "source": [
    "def cost_function(yhat, y):   \n",
    "    m = y.shape[1] # no. of examples\n",
    "  \n",
    "    product_sum = np.sum((y *np.log(yhat)), axis = 0)\n",
    "    cost = -1/m*np.sum(product_sum)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4sEmnKQzLBoB"
   },
   "outputs": [],
   "source": [
    "def agrregation_backward(dL_da, temp):\n",
    "    \n",
    "    h_prev, w, b = temp \n",
    "    m = h_prev.shape[1]\n",
    "    dL_dh_prev = np.dot(w.T, dL_da)\n",
    "    \n",
    "    dL_dw = 1/m*np.dot(dL_da, h_prev.T)\n",
    "    dL_db = 1/m*np.sum(dL_da, axis=1, keepdims=True)\n",
    "     \n",
    "\n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "k5_8wzplLBoC"
   },
   "outputs": [],
   "source": [
    "def activation_backward(dL_dh, temp, activation):\n",
    "\n",
    "    linear_temp, a = temp\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        ds = derivative_sigmoid(a)\n",
    "        dL_da = dL_dh * ds\n",
    "       \n",
    "        dL_dh_prev, dL_dw, dL_db = agrregation_backward(dL_da, linear_temp)    \n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dt = derivative_tanh(a)\n",
    "        dL_da = dL_dh * dt\n",
    "\n",
    "        dL_dh_prev, dL_dw, dL_db = agrregation_backward(dL_da, linear_temp)    \n",
    "    \n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Izs4-_QvLBoC"
   },
   "outputs": [],
   "source": [
    "def backward_pass(yhat, y, temps):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(temps) # the number of layers\n",
    "    m = y.shape[1]\n",
    "\n",
    "# el = one hot vector\n",
    "    el = y\n",
    "    dL_dyhat = -(1/yhat)*el\n",
    "    dL_daL  = -(el - yhat)\n",
    "    current_temp = temps[L-1]\n",
    "    linear_tempL,aL = current_temp\n",
    "    \n",
    "    hL_prev, wL, bL = linear_tempL\n",
    "    m = hL_prev.shape[1]\n",
    "\n",
    "    dL_dhL_prev = np.dot(wL.T, dL_daL)\n",
    "    \n",
    "    dL_dwL = 1/m*np.dot(dL_daL, hL_prev.T)\n",
    "    dL_dbL = 1/m*np.sum(dL_daL, axis=1, keepdims=True)\n",
    "\n",
    "    grads[\"dL_dh\" + str(L-1)] = dL_dhL_prev\n",
    "    grads[\"dL_dw\" + str(L)]      = dL_dwL\n",
    "    grads[\"dL_db\" + str(L)] = dL_dbL\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        #print(l)\n",
    "        current_temp = temps[l]\n",
    "        dL_dh_prev, dL_dw, dL_db = activation_backward(grads[\"dL_dh\" + str(l+1)], current_temp, \"sigmoid\")\n",
    "        grads[\"dL_dh\" + str(l)] = dL_dh_prev\n",
    "        grads[\"dL_dw\" + str(l + 1)] = dL_dw\n",
    "        grads[\"dL_db\" + str(l + 1)] = dL_db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SzwkIK5CLBoD"
   },
   "outputs": [],
   "source": [
    "def parameter_update_vanilla(parameters, grads,learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- learning_rate*grads[\"dL_dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- learning_rate*grads[\"dL_db\" + str(l + 1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_momentum(parameters, grads, update, learning_rate ,gamma):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        update[\"update_w\" + str(l+1)] = gamma*update[\"update_w\" + str(l+1)] + learning_rate*grads[\"dL_dw\" + str(l + 1)]\n",
    "        update[\"update_b\" + str(l+1)] = gamma*update[\"update_b\" + str(l+1)] + learning_rate*grads[\"dL_db\" + str(l + 1)]\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]-update[\"update_w\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- update[\"update_b\" + str(l+1)]\n",
    "\n",
    "    return parameters, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_RMSProp(parameters, grads, velocity, learning_rate ,beta,eps):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        velocity[\"v_w\" + str(l+1)] = beta*velocity[\"v_w\" + str(l+1)] + (1-beta)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta*velocity[\"v_b\" + str(l+1)] + (1-beta)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*grads[\"dL_dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*grads[\"dL_db\" + str(l + 1)]\n",
    "\n",
    "    return parameters, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_adam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,epoch):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        \n",
    "        moment[\"m_w\" + str(l+1)] = beta1*moment[\"m_w\" + str(l+1)] + (1-beta1)*grads[\"dL_dw\" + str(l + 1)]\n",
    "        moment[\"m_b\" + str(l+1)] = beta1*moment[\"m_b\" + str(l+1)] + (1-beta1)*grads[\"dL_db\" + str(l + 1)]\n",
    "           \n",
    "        velocity[\"v_w\" + str(l+1)] = beta2*velocity[\"v_w\" + str(l+1)] + (1-beta2)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta2*velocity[\"v_b\" + str(l+1)] + (1-beta2)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        m_w_hat = moment[\"m_w\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        m_b_hat = moment[\"m_b\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        \n",
    "        v_w_hat = velocity[\"v_w\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        v_b_hat = velocity[\"v_b\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- ((learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*m_w_hat)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- ((learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*m_b_hat)\n",
    "\n",
    "    return parameters, velocity, moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_nadam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,epoch):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        \n",
    "        moment[\"m_w\" + str(l+1)] = beta1*moment[\"m_w\" + str(l+1)] + (1-beta1)*grads[\"dL_dw\" + str(l + 1)]\n",
    "        moment[\"m_b\" + str(l+1)] = beta1*moment[\"m_b\" + str(l+1)] + (1-beta1)*grads[\"dL_db\" + str(l + 1)]\n",
    "           \n",
    "        velocity[\"v_w\" + str(l+1)] = beta2*velocity[\"v_w\" + str(l+1)] + (1-beta2)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta2*velocity[\"v_b\" + str(l+1)] + (1-beta2)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        m_w_hat = moment[\"m_w\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        m_b_hat = moment[\"m_b\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        \n",
    "        v_w_hat = velocity[\"v_w\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        v_b_hat = velocity[\"v_b\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        \n",
    "        \n",
    "        nadam_update_w =  (beta1*m_w_hat) + (((1-beta1)*grads[\"dL_dw\" + str(l + 1)])/ (1-beta1**(epoch+1)))\n",
    "        nadam_update_b =  (beta1*m_b_hat) + (((1-beta1)*grads[\"dL_db\" + str(l + 1)])/ (1-beta1**(epoch+1)))\n",
    "\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- ((learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*nadam_update_w)                                       \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- ((learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*nadam_update_b)\n",
    "\n",
    "    return parameters, velocity, moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lookahead_parameters(parameters,update,gamma):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    lookahead_parameters = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        lookahead_parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]-gamma*update[\"update_w\" + str(l+1)] \n",
    "        lookahead_parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-gamma*update[\"update_b\" + str(l+1)]\n",
    "    return lookahead_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "iDKqDY1dLBoD"
   },
   "outputs": [],
   "source": [
    "def predict(x, y, parameters):\n",
    "    x =x.T\n",
    "    y =y.T\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    prob, temps = forward_pass(x, parameters)\n",
    "    \n",
    "\n",
    "    predicted_label = np.argmax(prob, axis=0)\n",
    "    true_label = np.argmax(y, axis=0)\n",
    "    \n",
    "    Accuracy = np.sum(predicted_label == true_label)/m\n",
    "        \n",
    "#         if probas[0,i] > 0.5:\n",
    "#             p[0,i] = 1\n",
    "#         else:\n",
    "#             p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    #print(\"Accuracy: \"  + str(Accuracy))\n",
    "        \n",
    "    return Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RK_ze5mDLBoD",
    "outputId": "c7e09113-b4de-4448-d25c-92df6bdcdd7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_orig = x_train_orig[:5000]\n",
    "x_test_orig = x_test_orig[:1000]\n",
    "y_train_orig = y_train_orig[:5000]\n",
    "y_test_orig = y_train_orig[:1000]\n",
    "y_train_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vqejUVSLBoE",
    "outputId": "7198b220-8fdd-4216-c443-326e179f4330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape: (5000, 784)\n",
      "x_test's shape: (1000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1)\n",
    "x_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1)\n",
    "x_train = x_train_flatten/255\n",
    "x_test = x_test_flatten/255\n",
    "\n",
    "print (\"x_train's shape: \" + str(x_train.shape))\n",
    "print (\"x_test's shape: \" + str(x_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "MSTq1e4SLBoE"
   },
   "outputs": [],
   "source": [
    "# index = 15\n",
    "# plt.imshow(x_train_orig[index])\n",
    "# print (\"y = \" + str(y_train_orig[index])+ \". It's a \" + class_names[y_train_orig[index]] +  \" picture.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = np.array([[1,2,3,4],[3,4,5,6],[5,6,7,8]])\n",
    "y_train = np.array([[0,0,1],[1,0,0],[0,1,0]])\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "WVOCfRCQLBoG"
   },
   "outputs": [],
   "source": [
    "no_of_training_examples = np.shape(x_train)[0]\n",
    "no_of_testing_examples = np.shape(x_test)[0]\n",
    "size_input_layer = [x_train.shape[1]]\n",
    "no_hidden_layers = 3\n",
    "size_hidden_layers = [32,32,32]\n",
    "no_neurons_output_layer = x_train.shape[1] # no of class labels\n",
    "size_output_layer = [no_neurons_output_layer]   #no_neuron_output = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkU692HSLBoG",
    "outputId": "8db703a3-d2ba-46ec-ce95-07d531c7909d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = one_hot_vector_form(y_train_orig,no_neurons_output_layer)\n",
    "y_test = one_hot_vector_form(y_test_orig,no_neurons_output_layer)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNHTywuvLBoH",
    "outputId": "2c44494e-bce9-45f2-ee3d-658f8acae3af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 32, 32, 32, 784]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dimensions = size_input_layer + size_hidden_layers + size_output_layer\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\n",
    "            \"values\": [5,10]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [16,34,64]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [1e-3,1e-4]\n",
    "        },\n",
    "        \"number of hidden layers\": {\n",
    "            \"values\": [3,4,5]\n",
    "        },\n",
    "        \"size of every hidden layer\": {\n",
    "            \"values\": [32,64,128]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"Momentum\",\"NAG\",\"RMSPrpp\", \"Adam\",\"Nadam\"]\n",
    "        },\n",
    "        \"activation functions\": {\n",
    "            \"values\": [\"sigmoid\",\"tanh\"]\n",
    "        }    \n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validation_loss(x_train,y_train,x_valid,y_valid, parameters):\n",
    "    xt = x_train.T\n",
    "    yt = y_train.T\n",
    "    \n",
    "    xv = x_valid.T\n",
    "    yv = y_valid.T\n",
    "\n",
    "    yhatt, tempst = forward_pass(xt, parameters)\n",
    "    train_loss = cost_function(yhatt, yt)\n",
    "\n",
    "    yhatv, tempsv = forward_pass(xv, parameters)\n",
    "    valid_loss = cost_function(yhatv, yv)\n",
    "\n",
    "    return train_loss,valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "tFIBmzk1LBoH"
   },
   "outputs": [],
   "source": [
    "def L_layer_network(x_train, y_train,x_valid,y_valid,layer_dimensions,learning_rate,num_epochs,gamma = 0.9,  \n",
    "                    batch_type =\"Full_batch\",batchsize = 2,grad_descent_type = \"Vanilla\", beta_rms = 0.9, \n",
    "                    beta1 = 0.9,beta2 = 0.999,eps = 1e-8, print_cost=False):\n",
    "    \n",
    "    print(learning_rate)\n",
    "\n",
    "    #np.random.seed(1)\n",
    "    costs = []          \n",
    "    \n",
    "    parameters = initialize_parameters(layer_dimensions)\n",
    "    update = initialize_update(layer_dimensions)\n",
    "    velocity = initialize_velocity(layer_dimensions)\n",
    "    moment = initialize_moment(layer_dimensions)\n",
    "    if batch_type == \"SGD\":\n",
    "        batchsize =1\n",
    "    elif batch_type ==  \"Mini_batch\":\n",
    "        batchsize = batchsize\n",
    "    elif batch_type == \"Full_batch\":\n",
    "        batchsize = x_train.shape[0]\n",
    "        \n",
    "    total_examples = x_train.shape[0]\n",
    "    num_steps = total_examples//batchsize\n",
    "    #print(num_steps)\n",
    " \n",
    "    for i in range(0, num_epochs):\n",
    "        print(\"***********epoch = \",i)\n",
    "        par_update = 0\n",
    "        for j in range(num_steps):\n",
    "            \n",
    "            start = j*batchsize\n",
    "            end = start+batchsize\n",
    "            x = x_train[start:end].T\n",
    "            y = y_train[start:end].T \n",
    "            \n",
    "            #print(\"param\",parameters)\n",
    "            #print(\"in update\",update)\n",
    "               \n",
    "            if grad_descent_type == \"Vanilla\":\n",
    "            \n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters = parameter_update_vanilla(parameters, grads,learning_rate)\n",
    "                \n",
    "            elif grad_descent_type == \"Momentum\":\n",
    "                \n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                #print(\"in update\",update)\n",
    "                parameters,update = parameter_update_momentum(parameters, grads, update, learning_rate ,gamma)\n",
    "                #print(\"out update\",update)\n",
    "                \n",
    "            elif grad_descent_type == \"NAG\":\n",
    "                lookahead_parameters = find_lookahead_parameters(parameters,update,gamma)\n",
    "                #print(\"lookahead_parameters\",lookahead_parameters)\n",
    "                yhat, temps = forward_pass(x, lookahead_parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,update = parameter_update_momentum(parameters, grads, update, learning_rate ,gamma)\n",
    "                \n",
    "            elif grad_descent_type == \"RMSProp\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity= parameter_update_RMSProp(parameters, grads, velocity,learning_rate ,beta_rms,eps)\n",
    "                #print(\"velocity\",velocity)\n",
    "                \n",
    "            elif grad_descent_type == \"Adam\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity, moment = parameter_update_adam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,i)\n",
    "                #print(\"velocity\",velocity)\n",
    "\n",
    "            elif grad_descent_type == \"Nadam\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity, moment = parameter_update_nadam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,i)\n",
    "                #print(\"velocity\",velocity)   \n",
    "\n",
    "            #print(\"out update\",update)\n",
    "            par_update += 1\n",
    "            \n",
    "       \n",
    "        \n",
    "        #print(\"par_updated \",par_update,\"times\")\n",
    "        #print(\"***********************************************************\")\n",
    "        \n",
    "        \n",
    "    #print(\"cost in iteration \",i,\" is =\",cost)\n",
    "                \n",
    "            # Print the cost every 100 training example\n",
    "#         if print_cost and i % 2 == 0:\n",
    "#             print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "#         if print_cost and i % 2 == 0:\n",
    "#             costs.append(cost)\n",
    "   \n",
    "        train_loss,valid_loss = get_train_and_validation_loss(x_train,y_train,x_valid,y_valid, parameters)     \n",
    "\n",
    "        train_acc= predict(x_train, y_train, parameters)\n",
    "\n",
    "        valid_acc= predict(x_valid, y_valid, parameters)\n",
    "        \n",
    "    \n",
    "        print(\"train_acc\",train_acc)\n",
    "        print(\"valid_acc\",valid_acc)\n",
    "    \n",
    "        print(\"train_loss\",train_loss)\n",
    "        print(\"valid_loss\",valid_loss)\n",
    "    # plot the cost\n",
    "    \n",
    "        print(\"x.shape\",x.shape)\n",
    "        yhat2, temps2 = forward_pass(x_train.T, parameters)\n",
    "        cost2 = cost_function(yhat2, y_train.T)\n",
    "        print(\"loss2\",cost2)\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4gK1XfLLBoI",
    "outputId": "aaf77a51-a907-4e68-e1b3-5d3793c1400f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "***********epoch =  0\n",
      "loss 13.443076941495713\n",
      "train_acc 0.0\n",
      "valid_acc 0.0\n",
      "train_loss 8.551632970580796\n",
      "valid_loss 8.691165996724466\n",
      "x.shape (784, 5000)\n",
      "loss2 8.551632970580796\n",
      "***********epoch =  1\n",
      "loss 8.551632970580796\n",
      "train_acc 0.0\n",
      "valid_acc 0.0\n",
      "train_loss 6.6638265439659445\n",
      "valid_loss 6.977983621781346\n",
      "x.shape (784, 5000)\n",
      "loss2 6.6638265439659445\n",
      "***********epoch =  2\n",
      "loss 6.6638265439659445\n",
      "train_acc 0.0254\n",
      "valid_acc 0.021\n",
      "train_loss 5.70051993588494\n",
      "valid_loss 6.125838759076554\n",
      "x.shape (784, 5000)\n",
      "loss2 5.70051993588494\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters,loss = L_layer_network(x_train, y_train,x_test,y_test,layer_dimensions,0.001,3,batch_type =\"Full_batch\",batchsize = 10,grad_descent_type = \"Nadam\",print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.630187770037902"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6ZaDOFXELBoI",
    "outputId": "f4dd0bb2-b713-474c-d4b2-e449417426cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "predict(x_train, y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Tg0V3J9RLBoJ",
    "outputId": "3ec70517-246f-4afb-df38-6a503a21c787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.094\n"
     ]
    }
   ],
   "source": [
    "predict(x_test,y_test,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GopICBHzLBoJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q3A1_final_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
