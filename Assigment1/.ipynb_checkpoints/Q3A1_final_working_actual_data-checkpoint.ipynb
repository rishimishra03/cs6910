{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "35I0Rq_ALBn1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "95tMwn2GLBn-"
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    s = 1/(1+np.exp(-a))\n",
    "    return s\n",
    "\n",
    "def derivative_sigmoid(a):\n",
    "    ds = sigmoid(a) *(1-sigmoid (a))\n",
    "    return ds\n",
    "\n",
    "def tanh(a):\n",
    "    t=(np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "    return t\n",
    "\n",
    "def derivative_tanh(a):\n",
    "    dt=1-tanh(a)**2\n",
    "    return dt\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    return np.exp(a) / np.sum(np.exp(a), axis=0) #expA (axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "jPo8y7emLBn_"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaktHRmbLBn_",
    "outputId": "d34bec94-2a04-4235-81ff-4f96c7c87ead"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Pgc8UpKtLBn_"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        parameters['w' + str(k)] = np.random.randn(layer_dimensions[k], layer_dimensions[k-1]) \n",
    "        parameters['b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_update(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    update = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        update['update_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        update['update_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    velocity = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        velocity['v_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        velocity['v_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_moment(layer_dimensions):\n",
    "\n",
    "    #np.random.seed(0)\n",
    "    moment = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        moment['m_w' + str(k)] = np.zeros((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        moment['m_b' + str(k)] = np.zeros((layer_dimensions[k], 1))\n",
    "        \n",
    "    return moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "44uvQtw7LBoA"
   },
   "outputs": [],
   "source": [
    "def agrregation_forward(h, w, b):\n",
    "    \n",
    "    a = np.dot(w, h) + b\n",
    "    temp = (h,w,b)\n",
    "    \n",
    "    return a ,temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ynL63aRULBoA"
   },
   "outputs": [],
   "source": [
    "def activation_forward(h_prev, w, b, activation):\n",
    "        \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = sigmoid(a)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = tanh(a)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        a, linear_temp = agrregation_forward(h_prev, w, b)\n",
    "        h = softmax(a)\n",
    "    \n",
    "    \n",
    "    temp = (linear_temp, a)\n",
    "\n",
    "    return h, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "d7MK5JmxLBoB"
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, parameters):\n",
    "\n",
    "    temps = []\n",
    "    h = x\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for k in range(L-1):\n",
    "        l = k+1\n",
    "        h_prev = h \n",
    "        h,temp = activation_forward(h_prev, parameters['w'+str(l)], parameters['b'+str(l)], activation=\"sigmoid\")\n",
    "        temps.append(temp)\n",
    "    \n",
    "    \n",
    "    hL,temp1 = activation_forward(h, parameters['w'+str(L)], parameters['b'+str(L)], activation=\"softmax\")\n",
    "    temps.append(temp1)\n",
    "    \n",
    "            \n",
    "    return hL, temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "OX6qxo5yLBoB"
   },
   "outputs": [],
   "source": [
    "def cost_function(yhat, y):   \n",
    "    m = y.shape[1] # no. of examples\n",
    "  \n",
    "    product_sum = np.sum((y *np.log(yhat)), axis = 0)\n",
    "    cost = -1/m*np.sum(product_sum)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "4sEmnKQzLBoB"
   },
   "outputs": [],
   "source": [
    "def agrregation_backward(dL_da, temp):\n",
    "    \n",
    "    h_prev, w, b = temp \n",
    "    m = h_prev.shape[1]\n",
    "    dL_dh_prev = np.dot(w.T, dL_da)\n",
    "    \n",
    "    dL_dw = 1/m*np.dot(dL_da, h_prev.T)\n",
    "    dL_db = 1/m*np.sum(dL_da, axis=1, keepdims=True)\n",
    "     \n",
    "\n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "k5_8wzplLBoC"
   },
   "outputs": [],
   "source": [
    "def activation_backward(dL_dh, temp, activation):\n",
    "\n",
    "    linear_temp, a = temp\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        ds = derivative_sigmoid(a)\n",
    "        dL_da = dL_dh * ds\n",
    "       \n",
    "        dL_dh_prev, dL_dw, dL_db = agrregation_backward(dL_da, linear_temp)    \n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dt = derivative_tanh(a)\n",
    "        dL_da = dL_dh * dt\n",
    "\n",
    "        dL_dh_prev, dL_dw, dL_db = agrregation_backward(dL_da, linear_temp)    \n",
    "    \n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Izs4-_QvLBoC"
   },
   "outputs": [],
   "source": [
    "def backward_pass(yhat, y, temps):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(temps) # the number of layers\n",
    "    m = y.shape[1]\n",
    "\n",
    "# el = one hot vector\n",
    "    el = y\n",
    "    dL_dyhat = -(1/yhat)*el\n",
    "    dL_daL  = -(el - yhat)\n",
    "    current_temp = temps[L-1]\n",
    "    linear_tempL,aL = current_temp\n",
    "    \n",
    "    hL_prev, wL, bL = linear_tempL\n",
    "    m = hL_prev.shape[1]\n",
    "\n",
    "    dL_dhL_prev = np.dot(wL.T, dL_daL)\n",
    "    \n",
    "    dL_dwL = 1/m*np.dot(dL_daL, hL_prev.T)\n",
    "    dL_dbL = 1/m*np.sum(dL_daL, axis=1, keepdims=True)\n",
    "\n",
    "    grads[\"dL_dh\" + str(L-1)] = dL_dhL_prev\n",
    "    grads[\"dL_dw\" + str(L)]      = dL_dwL\n",
    "    grads[\"dL_db\" + str(L)] = dL_dbL\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        #print(l)\n",
    "        current_temp = temps[l]\n",
    "        dL_dh_prev, dL_dw, dL_db = activation_backward(grads[\"dL_dh\" + str(l+1)], current_temp, \"sigmoid\")\n",
    "        grads[\"dL_dh\" + str(l)] = dL_dh_prev\n",
    "        grads[\"dL_dw\" + str(l + 1)] = dL_dw\n",
    "        grads[\"dL_db\" + str(l + 1)] = dL_db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "SzwkIK5CLBoD"
   },
   "outputs": [],
   "source": [
    "def parameter_update_vanilla(parameters, grads,learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- learning_rate*grads[\"dL_dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- learning_rate*grads[\"dL_db\" + str(l + 1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_momentum(parameters, grads, update, learning_rate ,gamma):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        update[\"update_w\" + str(l+1)] = gamma*update[\"update_w\" + str(l+1)] + learning_rate*grads[\"dL_dw\" + str(l + 1)]\n",
    "        update[\"update_b\" + str(l+1)] = gamma*update[\"update_b\" + str(l+1)] + learning_rate*grads[\"dL_db\" + str(l + 1)]\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]-update[\"update_w\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- update[\"update_b\" + str(l+1)]\n",
    "\n",
    "    return parameters, update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_RMSProp(parameters, grads, velocity, learning_rate ,beta,eps):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        velocity[\"v_w\" + str(l+1)] = beta*velocity[\"v_w\" + str(l+1)] + (1-beta)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta*velocity[\"v_b\" + str(l+1)] + (1-beta)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*grads[\"dL_dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*grads[\"dL_db\" + str(l + 1)]\n",
    "\n",
    "    return parameters, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_adam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,epoch):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        \n",
    "        moment[\"m_w\" + str(l+1)] = beta1*moment[\"m_w\" + str(l+1)] + (1-beta1)*grads[\"dL_dw\" + str(l + 1)]\n",
    "        moment[\"m_b\" + str(l+1)] = beta1*moment[\"m_b\" + str(l+1)] + (1-beta1)*grads[\"dL_db\" + str(l + 1)]\n",
    "           \n",
    "        velocity[\"v_w\" + str(l+1)] = beta2*velocity[\"v_w\" + str(l+1)] + (1-beta2)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta2*velocity[\"v_b\" + str(l+1)] + (1-beta2)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        m_w_hat = moment[\"m_w\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        m_b_hat = moment[\"m_b\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        \n",
    "        v_w_hat = velocity[\"v_w\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        v_b_hat = velocity[\"v_b\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*m_w_hat\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*m_b_hat\n",
    "\n",
    "    return parameters, velocity, moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update_nadam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,epoch):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        \n",
    "        moment[\"m_w\" + str(l+1)] = beta1*moment[\"m_w\" + str(l+1)] + (1-beta1)*grads[\"dL_dw\" + str(l + 1)]\n",
    "        moment[\"m_b\" + str(l+1)] = beta1*moment[\"m_b\" + str(l+1)] + (1-beta1)*grads[\"dL_db\" + str(l + 1)]\n",
    "           \n",
    "        velocity[\"v_w\" + str(l+1)] = beta2*velocity[\"v_w\" + str(l+1)] + (1-beta2)*grads[\"dL_dw\" + str(l + 1)]**2\n",
    "        velocity[\"v_b\" + str(l+1)] = beta2*velocity[\"v_b\" + str(l+1)] + (1-beta2)*grads[\"dL_db\" + str(l + 1)]**2\n",
    "        \n",
    "        m_w_hat = moment[\"m_w\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        m_b_hat = moment[\"m_b\" + str(l+1)]/(1-beta1**(epoch+1))\n",
    "        \n",
    "        v_w_hat = velocity[\"v_w\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        v_b_hat = velocity[\"v_b\" + str(l+1)]/(1-beta2**(epoch+1))\n",
    "        \n",
    "        \n",
    "        nadam_update_w =  (beta1*m_w_hat) + (((1-beta1)*grads[\"dL_dw\" + str(l + 1)])/ (1-beta1**(epoch+1)))\n",
    "        nadam_update_b =  (beta1*m_b_hat) + (((1-beta1)*grads[\"dL_db\" + str(l + 1)])/ (1-beta1**(epoch+1)))\n",
    "\n",
    "        \n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_w\" + str(l+1)]+eps))*nadam_update_w                                       \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- (learning_rate / np.sqrt(velocity[\"v_b\" + str(l+1)]+eps))*nadam_update_b\n",
    "\n",
    "    return parameters, velocity, moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.pow(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lookahead_parameters(parameters,update,gamma):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    lookahead_parameters = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        lookahead_parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]-gamma*update[\"update_w\" + str(l+1)] \n",
    "        lookahead_parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-gamma*update[\"update_b\" + str(l+1)]\n",
    "    return lookahead_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "iDKqDY1dLBoD"
   },
   "outputs": [],
   "source": [
    "def predict(x, y, parameters):\n",
    "    x =x.T\n",
    "    y =y.T\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    prob, temps = forward_pass(x, parameters)\n",
    "    \n",
    "\n",
    "    predicted_label = np.argmax(prob, axis=0)\n",
    "    true_label = np.argmax(y, axis=0)\n",
    "    \n",
    "    Accuracy = np.sum(predicted_label == true_label)/m\n",
    "        \n",
    "#         if probas[0,i] > 0.5:\n",
    "#             p[0,i] = 1\n",
    "#         else:\n",
    "#             p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(Accuracy))\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RK_ze5mDLBoD",
    "outputId": "c7e09113-b4de-4448-d25c-92df6bdcdd7c"
   },
   "source": [
    "x_train_orig = x_train_orig[:10]\n",
    "x_test_orig = x_test_orig[:5]\n",
    "y_train_orig = y_train_orig[:10]\n",
    "y_test_orig = y_train_orig[:5]\n",
    "y_train_orig.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vqejUVSLBoE",
    "outputId": "7198b220-8fdd-4216-c443-326e179f4330"
   },
   "source": [
    "x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1)#.T\n",
    "x_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1)#.T\n",
    "x_train = x_train_flatten/255\n",
    "x_test = x_test_flatten/255\n",
    "\n",
    "print (\"x_train's shape: \" + str(x_train.shape))\n",
    "print (\"x_test's shape: \" + str(x_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "MSTq1e4SLBoE"
   },
   "outputs": [],
   "source": [
    "# index = 15\n",
    "# plt.imshow(x_train_orig[index])\n",
    "# print (\"y = \" + str(y_train_orig[index])+ \". It's a \" + class_names[y_train_orig[index]] +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array([[1,2,3,4],[3,4,5,6],[5,6,7,8]])\n",
    "y_train = np.array([[0,0,1],[1,0,0],[0,1,0]])\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "SwhPEvKjLBoF"
   },
   "outputs": [],
   "source": [
    "no_hidden_layers = 1 # no of hidden layers\n",
    "no_neuron_hidden = 4 # no. of neurons in each hidden layers\n",
    "no_neuron_output = 3 # # no. of neurons in each hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "WVOCfRCQLBoG"
   },
   "outputs": [],
   "source": [
    "no_of_training_examples = np.shape(x_train)[0]\n",
    "#no_of_testing_examples = np.shape(x_test)[0]\n",
    "size_input_layer = [x_train.shape[1]]\n",
    "no_hidden_layers = 2\n",
    "size_hidden_layers = [4,3]\n",
    "size_output_layer = [10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "OjczHGgOLBoG"
   },
   "outputs": [],
   "source": [
    "def one_hot_vector_form(labels,size_output_layer):\n",
    "    no_of_examples = labels.shape[0]\n",
    "    one_hot_vector = np.zeros((no_of_examples , size_output_layer))\n",
    "    for i in range(no_of_examples):\n",
    "        one_hot_vector[i, labels[i]] = 1    \n",
    "        y = one_hot_vector#.T\n",
    "    return y  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkU692HSLBoG",
    "outputId": "8db703a3-d2ba-46ec-ce95-07d531c7909d"
   },
   "source": [
    "y_train = one_hot_vector_form(y_train_orig,size_output_layer)\n",
    "y_test = one_hot_vector_form(y_test_orig,size_output_layer)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "Uy66Fnq-NBMK"
   },
   "source": [
    "num_steps = 4\n",
    "batchsize = 2\n",
    "for j in range(num_steps):\n",
    "  start = j*batchsize\n",
    "  end = start+batchsize\n",
    "  x = x_train[start:end].T\n",
    "  y = y_train[start:end].T \n",
    "            \n",
    "  #print(\"x\",x)\n",
    "  #print(\"Y\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNHTywuvLBoH",
    "outputId": "2c44494e-bce9-45f2-ee3d-658f8acae3af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 3, 10]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dimensions = size_input_layer + size_hidden_layers + size_output_layer\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v_w1': array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]),\n",
       " 'v_b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'v_w2': array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]),\n",
       " 'v_b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'v_w3': array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " 'v_b3': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "velo = initialize_velocity(layer_dimensions)\n",
    "velo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[ 0.605113  , -1.22244522,  0.57228786, -0.24796819],\n",
       "        [-0.68379726,  0.04230595,  0.05292313,  0.32519867],\n",
       "        [-2.40600739,  1.01802245, -0.75336393, -0.91916087],\n",
       "        [-1.70283982, -0.14775631, -0.9135634 , -1.61842618]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'w2': array([[-1.27372657, -0.09783771, -1.00431167,  2.03344773],\n",
       "        [ 0.43222517, -0.88064236, -0.12510805,  1.43812515],\n",
       "        [-0.71213434, -0.69159649, -1.72816224,  0.43877603]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters(layer_dimensions)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.42045201, -4.88978086,  2.28915144, -0.99187276],\n",
       "       [-2.73518904,  0.16922379,  0.21169254,  1.30079467],\n",
       "       [-9.62402954,  4.0720898 , -3.0134557 , -3.67664349],\n",
       "       [-6.81135927, -0.59102525, -3.6542536 , -6.47370471]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*(parameters[\"w1\"]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [3, 4, 5, 6],\n",
       "       [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.41421356, 1.73205081, 2.        , 2.23606798],\n",
       "       [2.        , 2.23606798, 2.44948974, 2.64575131],\n",
       "       [2.44948974, 2.64575131, 2.82842712, 3.        ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(x_train+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.41421356, 1.15470054, 1.        , 0.89442719],\n",
       "       [1.        , 0.89442719, 0.81649658, 0.75592895],\n",
       "       [0.81649658, 0.75592895, 0.70710678, 0.66666667]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd =2/np.sqrt(x_train+1)\n",
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.41421356, 2.30940108, 3.        , 3.57770876],\n",
       "       [3.        , 3.57770876, 4.0824829 , 4.53557368],\n",
       "       [4.0824829 , 4.53557368, 4.94974747, 5.33333333]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd*x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.37929966, 0.38516438, 0.38066278],\n",
       "        [0.33400188, 0.31763605, 0.31278171],\n",
       "        [0.28669846, 0.29719957, 0.30655551]]),\n",
       " [((array([[1, 3, 5],\n",
       "           [2, 4, 6],\n",
       "           [3, 5, 7],\n",
       "           [4, 6, 8]]),\n",
       "    array([[ 0.605113  , -1.22244522,  0.57228786, -0.24796819],\n",
       "           [-0.68379726,  0.04230595,  0.05292313,  0.32519867],\n",
       "           [-2.40600739,  1.01802245, -0.75336393, -0.91916087],\n",
       "           [-1.70283982, -0.14775631, -0.9135634 , -1.61842618]]),\n",
       "    array([[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]])),\n",
       "   array([[ -1.11478661,  -1.7008117 ,  -2.28683678],\n",
       "          [  0.86037871,   0.33363968,  -0.19309934],\n",
       "          [ -6.30669775, -12.42771723, -18.5487367 ],\n",
       "          [-11.21274735, -19.97791877, -28.74309019]])),\n",
       "  ((array([[2.46979594e-01, 1.54359283e-01, 9.22190162e-02],\n",
       "           [7.02739771e-01, 5.82644703e-01, 4.51874610e-01],\n",
       "           [1.82072567e-03, 4.00598433e-06, 8.79804143e-09],\n",
       "           [1.35008103e-05, 2.10717263e-09, 3.28877786e-13]]),\n",
       "    array([[-1.27372657, -0.09783771, -1.00431167,  2.03344773],\n",
       "           [ 0.43222517, -0.88064236, -0.12510805,  1.43812515],\n",
       "           [-0.71213434, -0.69159649, -1.72816224,  0.43877603]]),\n",
       "    array([[0.],\n",
       "           [0.],\n",
       "           [0.]])),\n",
       "   array([[-0.38514005, -0.25362016, -0.1616722 ],\n",
       "          [-0.51231998, -0.44638414, -0.35808054],\n",
       "          [-0.66503559, -0.5128865 , -0.37818724]]))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_pass(x_train.T, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "predict(x_train, y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "tFIBmzk1LBoH"
   },
   "outputs": [],
   "source": [
    "def L_layer_network(x_train, y_train,layer_dimensions,learning_rate,num_epochs,gamma = 0.9,  \n",
    "                    batch_type =\"Full_batch\",batchsize = 2,grad_descent_type = \"Vanilla\", beta_rms = 0.9, \n",
    "                    beta1 = 0.9,beta2 = 0.9,eps = 1e-8, print_cost=False):\n",
    "    \n",
    "    print(learning_rate)\n",
    "\n",
    "    #np.random.seed(1)\n",
    "    costs = []          \n",
    "    \n",
    "    parameters = initialize_parameters(layer_dimensions)\n",
    "    update = initialize_update(layer_dimensions)\n",
    "    velocity = initialize_velocity(layer_dimensions)\n",
    "    moment = initialize_moment(layer_dimensions)\n",
    "    if batch_type == \"SGD\":\n",
    "        batchsize =1\n",
    "    elif batch_type ==  \"Mini_batch\":\n",
    "        batchsize = batchsize\n",
    "    elif batch_type == \"Full_batch\":\n",
    "        batchsize = x_train.shape[0]\n",
    "        \n",
    "    total_examples = x_train.shape[0]\n",
    "    num_steps = total_examples//batchsize\n",
    "    #print(num_steps)\n",
    " \n",
    "    for i in range(0, num_epochs):\n",
    "        print(\"***********epoch = \",i)\n",
    "        par_update = 0\n",
    "        for j in range(num_steps):\n",
    "            \n",
    "            start = j*batchsize\n",
    "            end = start+batchsize\n",
    "            x = x_train[start:end].T\n",
    "            y = y_train[start:end].T \n",
    "            \n",
    "            #print(\"param\",parameters)\n",
    "            #print(\"in update\",update)\n",
    "               \n",
    "            if grad_descent_type == \"Vanilla\":\n",
    "            \n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters = parameter_update_vanilla(parameters, grads,learning_rate)\n",
    "                \n",
    "            elif grad_descent_type == \"Momentum\":\n",
    "                \n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                #print(\"in update\",update)\n",
    "                parameters,update = parameter_update_momentum(parameters, grads, update, learning_rate ,gamma)\n",
    "                #print(\"out update\",update)\n",
    "                \n",
    "            elif grad_descent_type == \"NAG\":\n",
    "                lookahead_parameters = find_lookahead_parameters(parameters,update,gamma)\n",
    "                #print(\"lookahead_parameters\",lookahead_parameters)\n",
    "                yhat, temps = forward_pass(x, lookahead_parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,update = parameter_update_momentum(parameters, grads, update, learning_rate ,gamma)\n",
    "                \n",
    "            elif grad_descent_type == \"RMSProp\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity= parameter_update_RMSProp(parameters, grads, velocity,learning_rate ,beta_rms,eps)\n",
    "                #print(\"velocity\",velocity)\n",
    "                \n",
    "            elif grad_descent_type == \"Adam\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity, moment = parameter_update_adam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,i)\n",
    "                #print(\"velocity\",velocity)\n",
    "\n",
    "            elif grad_descent_type == \"Nadam\":\n",
    "                yhat, temps = forward_pass(x, parameters)\n",
    "                cost = cost_function(yhat, y)\n",
    "                grads = backward_pass(yhat,y,temps)\n",
    "                parameters,velocity, moment = parameter_update_nadam(parameters, grads, velocity, moment,learning_rate ,beta1,beta2,eps,i)\n",
    "                #print(\"velocity\",velocity)   \n",
    "\n",
    "            #print(\"out update\",update)\n",
    "            par_update += 1\n",
    "        \n",
    "        print(\"par_updated \",par_update,\"times\")\n",
    "        print(\"***********************************************************\")\n",
    "        \n",
    "    #     #print(\"cost in iteration \",i,\" is =\",cost)\n",
    "                \n",
    "    #         # Print the cost every 100 training example\n",
    "    #     if print_cost and i % 100 == 0:\n",
    "    #         print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    #     if print_cost and i % 100 == 0:\n",
    "    #         costs.append(cost)\n",
    "    # # plot the cost\n",
    "    # plt.plot(np.squeeze(costs))\n",
    "    # plt.ylabel('cost')\n",
    "    # plt.xlabel('iterations (per hundreds)')\n",
    "    # plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    # plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4gK1XfLLBoI",
    "outputId": "aaf77a51-a907-4e68-e1b3-5d3793c1400f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "***********epoch =  0\n",
      "par_updated  1 times\n",
      "***********************************************************\n",
      "***********epoch =  1\n",
      "par_updated  1 times\n",
      "***********************************************************\n",
      "***********epoch =  2\n",
      "par_updated  1 times\n",
      "***********************************************************\n",
      "***********epoch =  3\n",
      "par_updated  1 times\n",
      "***********************************************************\n",
      "***********epoch =  4\n",
      "par_updated  1 times\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "parameters = L_layer_network(x_train, y_train, layer_dimensions,0.1,5,grad_descent_type = \"RMSProp\",print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[  2.26736466,   5.73005981,   6.18858314,   7.82924814],\n",
       "        [-73.18099915, -73.79981161, -74.05578235, -73.04065069],\n",
       "        [ -0.75683644,  -0.40210793,  -1.06098473,  -2.44007136],\n",
       "        [ -0.11135225,  -1.77045326,  -1.58715983,  -4.16049516]]),\n",
       " 'b1': array([[ 1.98860739e+00],\n",
       "        [-7.15296182e+01],\n",
       "        [-3.19285017e-02],\n",
       "        [-1.07653434e+00]]),\n",
       " 'w2': array([[ 35.62980241,  -0.65697496,  -2.56583185,  74.61087684],\n",
       "        [-36.52542684, -57.2859071 ,   1.66990971, -71.65807942],\n",
       "        [-34.43690199,  59.56155287,  -3.80831888, -71.97653527]]),\n",
       " 'b2': array([[ 37.7213183 ],\n",
       "        [-38.62825971],\n",
       "        [-36.79429088]])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZaDOFXELBoI",
    "outputId": "f4dd0bb2-b713-474c-d4b2-e449417426cb"
   },
   "outputs": [],
   "source": [
    "predict(x_train, y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass(x_train.T, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg0V3J9RLBoJ",
    "outputId": "3ec70517-246f-4afb-df38-6a503a21c787"
   },
   "outputs": [],
   "source": [
    "predict(x_test,y_test,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GopICBHzLBoJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q3A1_final_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
