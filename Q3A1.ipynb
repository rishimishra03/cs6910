{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    s = 1/(1+np.exp(-a))\n",
    "    return s\n",
    "\n",
    "def derivative_sigmoid(a):\n",
    "    ds = sigmoid(a) *(1-sigmoid (a))\n",
    "    return ds\n",
    "\n",
    "def tanh(a):\n",
    "    t=(np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "    return t\n",
    "\n",
    "def derivative_tanh(a):\n",
    "    dt=1-tanh(a)**2\n",
    "    return dt\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    return np.exp(a) / np.sum(np.exp(a), axis=0) #expA (axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = x_train.reshape(60000,784)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [3, 4]],\n",
       "\n",
       "       [[5, 6],\n",
       "        [7, 8]]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1,2], [3,4]],[[5,6], [7,8]]])\n",
    "bb = np.array([1,2])\n",
    "labels = bb\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = a.reshape(2,4)\n",
    "x= x.T\n",
    "np.shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5],\n",
       "       [2, 6],\n",
       "       [3, 7],\n",
       "       [4, 8]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01275478, 0.03467109, 0.69638749, 0.25618664])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([5,6,9,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0320586 , 0.0320586 ],\n",
       "       [0.08714432, 0.08714432],\n",
       "       [0.23688282, 0.23688282],\n",
       "       [0.64391426, 0.64391426]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hidden_layers = 2 # no of hidden layers\n",
    "no_neuron_hidden = 4 # no. of neurons in each hidden layers\n",
    "no_neuron_output = 3 # # no. of neurons in each hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_examples = np.shape(x)[1]\n",
    "size_input_layer = np.shape(x)[0]\n",
    "size_hidden_layer = no_neuron_hidden\n",
    "size_output_layer = no_neuron_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vector = np.zeros((no_of_examples , size_output_layer))\n",
    "\n",
    "for i in range(no_of_examples ):\n",
    "    one_hot_vector[i, labels[i]] = 1\n",
    "    \n",
    "y = one_hot_vector.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 4, 3]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dimensions = [size_input_layer]+ [size_hidden_layer]*no_hidden_layers+ [size_output_layer]\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dimensions):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dimensions)            # number of layers in the network\n",
    "\n",
    "    for k in range(1, L):\n",
    "        \n",
    "        parameters['w' + str(k)] = np.ones((layer_dimensions[k], layer_dimensions[k-1])) \n",
    "        parameters['b' + str(k)] = np.ones((layer_dimensions[k], 1))\n",
    "        \n",
    "#         assert(parameters['w' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "#         assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " 'b1': array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " 'w2': array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " 'b2': array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " 'w3': array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " 'b3': array([[1.],\n",
       "        [1.],\n",
       "        [1.]])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = initialize_parameters(layer_dimensions)\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_agrregation(h, w, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    h -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    w -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- the input of the activation function, also called pre-activation parameter \n",
    "    temp -- a python tuple containing \"h\", \"w\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.dot(w, h) + b\n",
    "\n",
    "    #assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    temp = (h,w,b)\n",
    "    \n",
    "    return a ,temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 5],\n",
       "        [2, 6],\n",
       "        [3, 7],\n",
       "        [4, 8]]),\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,temp = linear_agrregation(x,param['w1'],param['b1'])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(h_prev, w, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    h_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    w -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    h -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "\n",
    "        a, linear_temp = linear_agrregation(h_prev, w, b)\n",
    "        h = sigmoid(a)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        a, linear_temp = linear_agrregation(h_prev, w, b)\n",
    "        h = tanh(a)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        a, linear_temp = linear_agrregation(h_prev, w, b)\n",
    "        h = softmax(a)\n",
    "    \n",
    "    \n",
    "#     assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    temp = (linear_temp, a)\n",
    "\n",
    "    return h, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.9999833, 1.       ],\n",
       "        [0.9999833, 1.       ],\n",
       "        [0.9999833, 1.       ],\n",
       "        [0.9999833, 1.       ]]),\n",
       " ((array([[1, 5],\n",
       "          [2, 6],\n",
       "          [3, 7],\n",
       "          [4, 8]]),\n",
       "   array([[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "   array([[1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.]])),\n",
       "  array([[11., 27.],\n",
       "         [11., 27.],\n",
       "         [11., 27.],\n",
       "         [11., 27.]])))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation(x,param['w1'],param['b1'],\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(3-1)):\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    temps = []\n",
    "    h = x\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for k in range(L-1):\n",
    "        l = k+1\n",
    "        h_prev = h \n",
    "        h,temp = activation(h_prev, parameters['w'+str(l)], parameters['b'+str(l)], activation=\"sigmoid\")\n",
    "        temps.append(temp)\n",
    "    \n",
    "    \n",
    "    hL,temp1 = activation(h, parameters['w'+str(L)], parameters['b'+str(L)], activation=\"softmax\")\n",
    "    temps.append(temp1)\n",
    "    \n",
    "    #assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return hL, temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat, temps = forward_pass(x, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((array([[1, 5],\n",
       "          [2, 6],\n",
       "          [3, 7],\n",
       "          [4, 8]]),\n",
       "   array([[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "   array([[1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.]])),\n",
       "  array([[11., 27.],\n",
       "         [11., 27.],\n",
       "         [11., 27.],\n",
       "         [11., 27.]])),\n",
       " ((array([[0.9999833, 1.       ],\n",
       "          [0.9999833, 1.       ],\n",
       "          [0.9999833, 1.       ],\n",
       "          [0.9999833, 1.       ]]),\n",
       "   array([[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "   array([[1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.]])),\n",
       "  array([[4.99993319, 5.        ],\n",
       "         [4.99993319, 5.        ],\n",
       "         [4.99993319, 5.        ],\n",
       "         [4.99993319, 5.        ]])),\n",
       " ((array([[0.9933067 , 0.99330715],\n",
       "          [0.9933067 , 0.99330715],\n",
       "          [0.9933067 , 0.99330715],\n",
       "          [0.9933067 , 0.99330715]]),\n",
       "   array([[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]),\n",
       "   array([[1.],\n",
       "          [1.],\n",
       "          [1.]])),\n",
       "  array([[4.97322682, 4.9732286 ],\n",
       "         [4.97322682, 4.9732286 ],\n",
       "         [4.97322682, 4.9732286 ]]))]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.09861229, -1.09861229],\n",
       "       [-1.09861229, -1.09861229],\n",
       "       [-1.09861229, -1.09861229]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [3., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y*(1/yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.        , -0.        ],\n",
       "       [-1.09861229, -0.        ],\n",
       "       [-0.        , -1.09861229]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = y1*np.log(yhat)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.09861229, -1.09861229])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(u,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.09861229, -1.09861229])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = np.sum((y *np.log(yhat)), axis = 0)\n",
    "prod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0986122886681098"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(prod)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(yhat, y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    hL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = y.shape[1] # no. of examples\n",
    "  \n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    product_sum = np.sum((y *np.log(yhat)), axis = 0)\n",
    "    cost = -1/m*np.sum(product_sum)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    #assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = cost_function(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dL_da, temp):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    h_prev, w, b = temp\n",
    "    m = h_prev.shape[1]\n",
    "\n",
    "    dL_dh_prev = np.dot(w.T, dL_da)\n",
    "    \n",
    "    dL_dw = 1/m*np.dot(dL_da, h_prev.T)\n",
    "    dL_db = 1/m*np.sum(dL_da, axis=1, keepdims=True)\n",
    "     \n",
    "    \n",
    "#     assert (dA_prev.shape == A_prev.shape)\n",
    "#     assert (dW.shape == W.shape)\n",
    "#     assert (db.shape == b.shape)\n",
    "    \n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dL_dh, temp, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dL_dh -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dL_dh_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_temp, a = temp\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        ds = derivative_sigmoid(a)\n",
    "        dL_da = dL_dh * ds\n",
    "       \n",
    "        dL_dh_prev, dL_dw, dL_db = linear_backward(dL_da, linear_temp)    \n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dt = derivative_tanh(a)\n",
    "        dL_da = dL_dh * dt\n",
    "\n",
    "        dL_dh_prev, dL_dw, dL_db = linear_backward(dL_da, linear_temp)    \n",
    "    \n",
    "    return dL_dh_prev, dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(yhat, y, temps):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l],\n",
    "                for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(temps) # the number of layers\n",
    "    m = y.shape[1]\n",
    "#     Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "\n",
    "# el = one hot vector\n",
    "    el = y\n",
    "    dL_dyhat = -(1/yhat)*el\n",
    "    dL_daL  = -(el - yhat)\n",
    "    current_temp = temps[L-1]\n",
    "    linear_tempL,aL = current_temp\n",
    "    \n",
    "    hL_prev, wL, bL = linear_tempL\n",
    "    m = hL_prev.shape[1]\n",
    "\n",
    "    dL_dhL_prev = np.dot(wL.T, dL_daL)\n",
    "    \n",
    "    dL_dwL = 1/m*np.dot(dL_daL, hL_prev.T)\n",
    "    dL_dbL = 1/m*np.sum(dL_daL, axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "    grads[\"dL_dh\" + str(L-1)] = dL_dhL_prev\n",
    "    grads[\"dL_dw\" + str(L)]      = dL_dwL\n",
    "    grads[\"dL_db\" + str(L)] = dL_dbL\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        print(l)\n",
    "        current_temp = temps[l]\n",
    "        dL_dh_prev, dL_dw, dL_db = linear_activation_backward(grads[\"dL_dh\" + str(l+1)], current_temp, \"sigmoid\")\n",
    "        grads[\"dL_dh\" + str(l)] = dL_dh_prev\n",
    "        grads[\"dL_dw\" + str(l + 1)] = dL_dw\n",
    "        grads[\"dL_db\" + str(l + 1)] = dL_db\n",
    "        #print(grads)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "grads = L_model_backward(yhat, y, temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dL_dh2': array([[-1.11022302e-16, -1.11022302e-16],\n",
       "        [-1.11022302e-16, -1.11022302e-16],\n",
       "        [-1.11022302e-16, -1.11022302e-16],\n",
       "        [-1.11022302e-16, -1.11022302e-16]]),\n",
       " 'dL_dw3': array([[ 0.33110231,  0.33110231,  0.33110231,  0.33110231],\n",
       "        [-0.16555104, -0.16555104, -0.16555104, -0.16555104],\n",
       "        [-0.16555127, -0.16555127, -0.16555127, -0.16555127]]),\n",
       " 'dL_db3': array([[ 0.33333333],\n",
       "        [-0.16666667],\n",
       "        [-0.16666667]]),\n",
       " 'dL_dh1': array([[-2.95252483e-18, -2.95233023e-18],\n",
       "        [-2.95252483e-18, -2.95233023e-18],\n",
       "        [-2.95252483e-18, -2.95233023e-18],\n",
       "        [-2.95252483e-18, -2.95233023e-18]]),\n",
       " 'dL_dw2': array([[-7.38100719e-19, -7.38100719e-19, -7.38100719e-19,\n",
       "         -7.38100719e-19],\n",
       "        [-7.38100719e-19, -7.38100719e-19, -7.38100719e-19,\n",
       "         -7.38100719e-19],\n",
       "        [-7.38100719e-19, -7.38100719e-19, -7.38100719e-19,\n",
       "         -7.38100719e-19],\n",
       "        [-7.38100719e-19, -7.38100719e-19, -7.38100719e-19,\n",
       "         -7.38100719e-19]]),\n",
       " 'dL_db2': array([[-7.38106883e-19],\n",
       "        [-7.38106883e-19],\n",
       "        [-7.38106883e-19],\n",
       "        [-7.38106883e-19]]),\n",
       " 'dL_dh0': array([[-1.97242157e-22, -2.21968892e-29],\n",
       "        [-1.97242157e-22, -2.21968892e-29],\n",
       "        [-1.97242157e-22, -2.21968892e-29],\n",
       "        [-1.97242157e-22, -2.21968892e-29]]),\n",
       " 'dL_dw1': array([[-2.46552835e-23, -4.93105558e-23, -7.39658282e-23,\n",
       "         -9.86211006e-23],\n",
       "        [-2.46552835e-23, -4.93105558e-23, -7.39658282e-23,\n",
       "         -9.86211006e-23],\n",
       "        [-2.46552835e-23, -4.93105558e-23, -7.39658282e-23,\n",
       "         -9.86211006e-23],\n",
       "        [-2.46552835e-23, -4.93105558e-23, -7.39658282e-23,\n",
       "         -9.86211006e-23]]),\n",
       " 'dL_db1': array([[-2.46552724e-23],\n",
       "        [-2.46552724e-23],\n",
       "        [-2.46552724e-23],\n",
       "        [-2.46552724e-23]])}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)]-learning_rate*grads[\"dL_dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"dL_db\" + str(l + 1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " 'b1': array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " 'w2': array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " 'b2': array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " 'w3': array([[0.96688977, 0.96688977, 0.96688977, 0.96688977],\n",
       "        [1.0165551 , 1.0165551 , 1.0165551 , 1.0165551 ],\n",
       "        [1.01655513, 1.01655513, 1.01655513, 1.01655513]]),\n",
       " 'b3': array([[0.96666667],\n",
       "        [1.01666667],\n",
       "        [1.01666667]])}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "parame = update_parameters(param, grads, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " 'b1': array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " 'w2': array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]),\n",
       " 'b2': array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " 'w3': array([[-2.50968448, -2.50968448, -2.50968448, -2.50968448],\n",
       "        [ 2.75484106,  2.75484106,  2.75484106,  2.75484106],\n",
       "        [ 2.75484341,  2.75484341,  2.75484341,  2.75484341]]),\n",
       " 'b3': array([[-2.53333333],\n",
       "        [ 2.76666667],\n",
       "        [ 2.76666667]])}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape: (784, 60000)\n",
      "x_test's shape: (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1).T\n",
    "x_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1).T\n",
    "x_train = x_train_flatten/255\n",
    "x_test = x_test_flatten/255\n",
    "\n",
    "print (\"x_train's shape: \" + str(x_train.shape))\n",
    "print (\"x_test's shape: \" + str(x_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 9. It's a Ankle boot picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASB0lEQVR4nO3dfWyd1X0H8O/X9/olsZNgxyQxISQBBdqUQQAvlBdlbBQGaFNg1RAgQSZozR+lFK2tiFppzT/r0LaWTVPHFBpGWBmoU8vItKg0pEwZW3lxUEoIAQIhgXiOnTRvdl4c+/q3P/ykMuDze8x9e25zvh/Jsv38fO49vrnfPNf3POccmhlE5PRXl3UHRKQ6FHaRSCjsIpFQ2EUiobCLRCJfzTtrYKM1obmadykSlRM4ipM2xIlqJYWd5A0A/h5ADsAPzewh7+eb0IzLeW0pdykijpdtY7BW9Mt4kjkAPwBwI4DFAG4nubjY2xORyirlb/alAN41s51mdhLA0wCWl6dbIlJupYR9LoAPx32/Jzn2ESS7SHaT7B7GUAl3JyKlqPi78Wa22sw6zayzHo2VvjsRCSgl7D0A5o37/uzkmIjUoFLC/iqARSQXkmwAcBuAdeXploiUW9FDb2Y2QvI+AM9hbOjtMTPbVraeiUhZlTTObmbrAawvU19EpIJ0uaxIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0SipF1c5fS3v+sKt75oxdtu/ZUdC4O19v9qcNu2Pv5Lty6fTklhJ7kLwACAAoARM+ssR6dEpPzKcWb/fTPbX4bbEZEK0t/sIpEoNewG4OckN5PsmugHSHaR7CbZPYyhEu9ORIpV6sv4q82sh+QsABtIvmVmm8b/gJmtBrAaAKazzUq8PxEpUklndjPrST73A3gGwNJydEpEyq/osJNsJjnt1NcArgfwRrk6JiLlVcrL+NkAniF56nb+1cx+VpZeSc040Dni1udPPeDW5/zOkWDt767vdtsuXPYlt37+3X77SsqdMcOtb//uBW69sf14sLbgznfctjZU3HtfRYfdzHYCuLjY9iJSXRp6E4mEwi4SCYVdJBIKu0gkFHaRSGiK62mA+fA/o434Q2dp/uSyzW79/aMz3fq8KQeDtbt2L/Nv+4YfuvVr/+Aet57/hd93T272LLd+9Ybdbv3+qS+59Tm58JDk/Td+1W075d9fceshOrOLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpHQOPtpwEaLXwAo3zHHrf9p63+69UeHf8+tT8+fCNZ2H2tz2z5xpN2tb/zRGrd+xa++GKzt/dC/7/f/6FG3/vRAq1vfNOBPcT2vqT9Ya+qrzPJtOrOLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpHQOPvpYLRQdNP37z7XrW8bmuvW83X+fQ+Nhp9in5nW57btHfbHslcf9rd8/rfPrQ3Wzr64xW37Dwfnu/XDhSlu/fwpe936WfnwPP+BBf5tTy9yJ2ud2UUiobCLREJhF4mEwi4SCYVdJBIKu0gkFHaRSGicPXL/dM8/uvW3hs5y6+dO2e/WBwpNwVqOo27b9vyAW09r/+zgZ4O1UfPPcx8M+fPdZzWE130HgBOj9W59el14nn//Urcppj/l10NSz+wkHyPZT/KNccfaSG4guSP57F/9ICKZm8zL+McB3PCxYysBbDSzRQA2Jt+LSA1LDbuZbQJw4GOHlwM4dS3iWgA3l7lfIlJmxf7NPtvMepOv9wKYHfpBkl0AugCgCVOLvDsRKVXJ78abmQEIrnhoZqvNrNPMOuvRWOrdiUiRig17H8kOAEg+h5fKFJGaUGzY1wFYkXy9AsCz5emOiFRK6t/sJJ8CcA2AdpJ7AHwHwEMAfkzyHgC7AdxayU5Gj/TrFl43vu7Cz7hNlzVtcesvDJzh1tvr/bFwb5y9PT/otk0bRx9ImVM+tS68/vq0/HG37fZjHW69/+R0tz6YC//eALC4qSdYu/iy99y2R91qWGrYzez2QOnaIu9TRDKgy2VFIqGwi0RCYReJhMIuEgmFXSQSmuJaA+qmTXPro0eP+Tdg4eWcd6/y/4lfGRp26x8c96d6NtX57esZ7ls9R9y2abyhNQBoduq7T/rbQTfn/Nuemjvp1mfV+1Ngdzn3/8/nrnPb3oor3HqIzuwikVDYRSKhsItEQmEXiYTCLhIJhV0kEgq7SCQ0zl4NKVNURwf8aaJpDt0VHnd988pH3LZPHPGnci6cWvxS0YA/TXXY/KffcMGvp43T7xsJT0M9ONzstu1oOOzWZ+RSrn1Ise342cHaXdNTHvPbPh+sjT73UrCmM7tIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMIuEonfrnF2b7ya/v9brEtZjjmFFcLzsr2lnCdVT7HzIX/+8qY7/iZY+8Gh8LbFADCtzl9SuTFlvvrBYX9Lr5Z8eF54peezD4yGnxN19P9N0rZcrqe/u1FdyjLYoxZ+PvYX/MWi918UbjuyyemTe6sictpQ2EUiobCLREJhF4mEwi4SCYVdJBIKu0gkfrvG2b3xamftdAAwf9izog6u8MfJv7TS396+a4Y/J/27+y8L1oZS5ozDH07G7uMp66s74+iAv376sVF/rLoA/9qI1PnwlgvWptb5677PyPvz1XtP+ltZp99++PqGvYVwvwFguDX8XLecs323e6sASD5Gsp/kG+OOrSLZQ3JL8nFT2u2ISLYm8zL+cQA3THD8YTNbknysL2+3RKTcUsNuZpsAHKhCX0Skgkp5g+4+kq8nL/NbQz9EsotkN8nuYfh/34lI5RQb9kcAnAdgCYBeAN8L/aCZrTazTjPrrIf/hoyIVE5RYTezPjMrmNkogEcBLC1vt0Sk3IoKO8nx6w/fAuCN0M+KSG1IHWcn+RSAawC0k9wD4DsAriG5BIAB2AXg3gr2sSzyc89y671/PN+tH7woPLb51WXPu23/vM0fJ19zeI5bf7BviVv3xnTT9gkfTFn3PW1edinS5soPpcwpT9MzFB4Lb633x9G9MXoAGBr1o3NkxH9cW5z938+o8+f586RzjnbmyaeG3cxun+DwmrR2IlJbdLmsSCQUdpFIKOwikVDYRSKhsItEoqamuB675XK3Pv+bbwdr17f5Q/1XTXnRrf/sqL/k8nkN/cHaB8Mz3bZf/vAqt+4tKwwA0+pPFN1+sOBftXhOoz/tYXrev++0IaZdJ8JTZKfm/GmgjSlLTfcVwlsyA0C+ztsu2h9ae+eYPxyapz+lekrOH1b07v+cfIvbdvbL4do+ZxVqndlFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUhUdZydDQ3Izz0nWL/tL/11K73pmN2DC922afW0KYs7c2cGa0dGprhtP9fS49Z7hoKrek1KvTPmm7Y1cdp4c5pSbn/vkD9OPmr+uajn2Ay33lIfnkZ6Zet7btu0Mf6DI/5W1R0Nh9x6W24wWOsdCdcAoHX99mAtfyR8XYTO7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJKo6zn5idj3eeiC8pPMduf9x2+88Hh7rntUw4LYtpIzZpo03f3A8PBbe0eQv15y2JPLcxoNuPW28+dhoQ7B2IuW++4b9se7jBb/9zHpnAjWAJme5aG+pZyB9Lv11rf4aBtdM+b9g7b+PdwRrAPDCMX99gx1Hws9FAHh+8AK3XnDWIHhymv9cxqE9wZI5W5frzC4SCYVdJBIKu0gkFHaRSCjsIpFQ2EUiobCLRKK668Yb4E0TXlC/z23e1xiev3y44M8pb8/7c4QL8NduP7shvL56qXPGD440+/Vhf+60N45f6rbIMxv8xy3td2urC4/D/+EZW922i+p/7dbvfecOt/5Xz84N1gpf8K9t8J8NwPCI/3vncv5W1zOmhK8huGBGn9s2vHuCL/XMTnIeyRdIvklyG8mvJcfbSG4guSP5XNoKDCJSUZN5GT8C4OtmthjA5wF8heRiACsBbDSzRQA2Jt+LSI1KDbuZ9ZrZa8nXAwC2A5gLYDmAtcmPrQVwc6U6KSKl+1Rv0JFcAOASAC8DmG1mvUlpL4DZgTZdJLtJdheO+tdRi0jlTDrsJFsA/ATAA2b2kZkfZmYAJnyXysxWm1mnmXXmmv03okSkciYVdpL1GAv6k2b20+RwH8mOpN4BILzNqYhkLnXojSQBrAGw3cy+P660DsAKAA8ln59Nu63GPUdx3jdeCtbvbP+y2/7+390YrC1recttu2PI34IX5j8UHwyFt2VO2xY5bRpo2jLW3tbDY/XwkslHR/y+jaYMMqUtqXxp8y63ftIZmvvGj+52256z6n/degN2u/VZTr35Vn+K6tv7Zrn1ujp/uHVoyP83PZ4P19Om9gLFLf89mXH2qwDcCWAryS3JsW9hLOQ/JnkPgN0Abi2qByJSFalhN7MXEb7G4NrydkdEKkWXy4pEQmEXiYTCLhIJhV0kEgq7SCQ4dvFbdUxnm13O4t/AZ2N4zPj9v7jUbXvv8ufc+hda3nTrFzWEt4vek7LF7s6RFrd+qOBPYd074i+53OyMs5+Z85e5vn6qPwU2bfvgZU99062f++Av3XpWvr1zi1vfemKeW0+b2jsnf9itbz66IFi7brq/RPbDF14WrL10Yj0Oj/56wtEzndlFIqGwi0RCYReJhMIuEgmFXSQSCrtIJBR2kUjU1jh7Xco83dHwdrSVdvSLlwdr+y7x/8/Mf9Yf677srA/d+vnNxa8LkrYM9X9sCP9eALBwZYbj5BV8Pux94Eq3nj/m5yJ30r/9hkF/DYLGA+F1AvK/2OzfuONl24gjdkDj7CIxU9hFIqGwi0RCYReJhMIuEgmFXSQSCrtIJGprnF1ESqJxdhFR2EViobCLREJhF4mEwi4SCYVdJBIKu0gkUsNOch7JF0i+SXIbya8lx1eR7CG5Jfm4qfLdFZFiTWZ/9hEAXzez10hOA7CZ5Iak9rCZ/W3luici5TKZ/dl7AfQmXw+Q3A5gbqU7JiLl9an+Zie5AMAlAF5ODt1H8nWSj5FsDbTpItlNsnsY4W2KRKSyJh12ki0AfgLgATM7AuARAOcBWIKxM//3JmpnZqvNrNPMOusR3qtNRCprUmEnWY+xoD9pZj8FADPrM7OCmY0CeBTA0sp1U0RKNZl34wlgDYDtZvb9ccc7xv3YLQD8rSdFJFOTeTf+KgB3AthK8tQ+t98CcDvJJQAMwC4A91akhyJSFpN5N/5FABPNj11f/u6ISKXoCjqRSCjsIpFQ2EUiobCLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpFQ2EUiobCLREJhF4mEwi4Siapu2UxyH4Dd4w61A9hftQ58OrXat1rtF6C+FaucfZtvZmdOVKhq2D9x52S3mXVm1gFHrfatVvsFqG/Fqlbf9DJeJBIKu0gksg776ozv31OrfavVfgHqW7Gq0rdM/2YXkerJ+swuIlWisItEIpOwk7yB5Nsk3yW5Mos+hJDcRXJrsg11d8Z9eYxkP8k3xh1rI7mB5I7k84R77GXUt5rYxtvZZjzTxy7r7c+r/jc7yRyAdwBcB2APgFcB3G5mb1a1IwEkdwHoNLPML8AguQzAIIAnzOzC5NhfAzhgZg8l/1G2mtmDNdK3VQAGs97GO9mtqGP8NuMAbgbwZ8jwsXP6dSuq8LhlcWZfCuBdM9tpZicBPA1geQb9qHlmtgnAgY8dXg5gbfL1Wow9Waou0LeaYGa9ZvZa8vUAgFPbjGf62Dn9qooswj4XwIfjvt+D2trv3QD8nORmkl1Zd2YCs82sN/l6L4DZWXZmAqnbeFfTx7YZr5nHrpjtz0ulN+g+6WozuxTAjQC+krxcrUk29jdYLY2dTmob72qZYJvx38jysSt2+/NSZRH2HgDzxn1/dnKsJphZT/K5H8AzqL2tqPtO7aCbfO7PuD+/UUvbeE+0zThq4LHLcvvzLML+KoBFJBeSbABwG4B1GfTjE0g2J2+cgGQzgOtRe1tRrwOwIvl6BYBnM+zLR9TKNt6hbcaR8WOX+fbnZlb1DwA3Yewd+fcAfDuLPgT6dS6AXyUf27LuG4CnMPaybhhj723cA2AmgI0AdgB4HkBbDfXtXwBsBfA6xoLVkVHfrsbYS/TXAWxJPm7K+rFz+lWVx02Xy4pEQm/QiURCYReJhMIuEgmFXSQSCrtIJBR2kUgo7CKR+H+4UYhhmXn+QAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 15\n",
    "plt.imshow(x_train_orig[index])\n",
    "print (\"y = \" + str(y_train_orig[index])+ \". It's a \" + class_names[y_train_orig[index]] +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hidden_layers = 3 # no of hidden layers\n",
    "no_neuron_hidden = 784 # no. of neurons in each hidden layers\n",
    "no_neuron_output = 10 # # no. of neurons in each hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_training_examples = np.shape(x_train)[1]\n",
    "no_of_testing_examples = np.shape(x_test)[1]\n",
    "size_input_layer = np.shape(x_train)[0]\n",
    "size_hidden_layer = no_neuron_hidden\n",
    "size_output_layer = no_neuron_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_orig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector_form(labels,size_output_layer):\n",
    "    no_of_examples = labels.shape[0]\n",
    "    one_hot_vector = np.zeros((no_of_examples , size_output_layer))\n",
    "    for i in range(no_of_examples):\n",
    "        one_hot_vector[i, labels[i]] = 1    \n",
    "        y = one_hot_vector.T\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = one_hot_vector_form(y_train_orig,size_output_layer)\n",
    "y_test = one_hot_vector_form(y_test_orig,size_output_layer)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 784, 784, 784, 10]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dimensions = [size_input_layer]+ [size_hidden_layer]*no_hidden_layers+ [size_output_layer]\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dimensions, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "   \n",
    "    parameters = initialize_parameters(layer_dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "       \n",
    "        yhat, temps = forward_pass(X, parameters)\n",
    " \n",
    "        cost = cost_function(yhat, y)\n",
    "        \n",
    "        grads = backward_pass(yhat,y,temps)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2) (10,60000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-332-368c78433588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-331-945ffb12c737>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layer_dimensions, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-201-1da7275b8b3c>\u001b[0m in \u001b[0;36mcost_function\u001b[0;34m(yhat, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Compute loss from aL and y.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mproduct_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (10,60000) "
     ]
    }
   ],
   "source": [
    "parameters = L_layer_model(x_train, y_train, layer_dimensions, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
